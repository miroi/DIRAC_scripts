#!/bin/bash
#SBATCH -J CFOUR

# Working directory on the shared storage ...
###SBATCH -D /lustre/nyx/ukt/milias

## max. execution time
#SBATCH -t 0-2:30:00

## Node count - take only one node for OpenMP-MKL
##  lxbk0051-lxbk0200 ...
#SBATCH -N 1
## CPU count  - max. 40 per node
#SBATCH -n 8

## memory
#SBATCH --mem=32G

## stdout/stderr output file
#SBATCH -o log_slurm_job.%j.%N.std_out_err

## mail
#SBATCH --mail-user=m.ilias@gsi.de 

 
echo Job user is $SLURM_JOB_USER and his job $SLURM_JOB_NAME has assigned ID $SLURM_JOBID
echo This job was submitted from the computer $SLURM_SUBMIT_HOST
echo and from the home directory:
echo $SLURM_SUBMIT_DIR
echo
echo It is running on the cluster compute node:
echo $SLURM_CLUSTER_NAME
echo and is employing $SLURM_JOB_NUM_NODES node/nodes:
echo $SLURM_JOB_NODELIST
echo The job requests $SLURM_CPUS_ON_NODE CPU per task.

module use /cvmfs/it.gsi.de/modulefiles/
echo "modules at disposal:"
module avail
echo
module unload compiler/intel/12.1
module unload compiler/intel/15.0
module load compiler/intel/15.0
echo "loaded modules:"
module list

echo -e "\nRunning on host `hostname`"
echo -e "Time is `date` \n"
echo -e "\nMy PATH=$PATH\n"

# CPU model, total numer of CPUs, number of allocated CPUs
echo -e "The node's CPU \c"; cat /proc/cpuinfo | grep 'model name' | uniq
NPROCS=`cat /proc/cpuinfo | grep processor | wc -l`
echo "This node has $NPROCS CPUs available."
echo "This node has $SLURM_CPUS_ON_NODE CPUs allocated for SLURM calculations."

echo -e "\n the memory at the node (in GB)"
free -t -g
echo

## set internal OpenMP parallelization for MKL - Slurm CPU count
export MKL_NUM_THREADS=$SLURM_CPUS_ON_NODE
#export MKL_NUM_THREADS=2
echo -e  "\nInternal MKL parallelization upon SLURM CPU count, MKL_NUM_THREADS=$MKL_NUM_THREADS\n"
export OMP_NUM_THREADS=1
export MKL_DYNAMIC="FALSE"
export OMP_DYNAMIC="FALSE"

export CFOUR=/lustre/nyx/ukt/milias/work/software/cfour/cfour_v2.00beta_64bit_linux_serial
export PATH=$CFOUR/bin:$PATH
cd $CFOUR/testsuite

#xtester --help
#xtester --testcase 001
xtester --all

# for running jobs from your homedir
#cd $SLURM_SUBMIT_DIR

exit
