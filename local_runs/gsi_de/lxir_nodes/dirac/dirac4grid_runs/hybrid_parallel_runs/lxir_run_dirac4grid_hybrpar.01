#!/bin/bash

#
# script for interactive job runs on Valeria private GSI nodes
#
#  lxir022-lxir026
#  lxir071-lxir074
#
# Launched as:
# ~~~~~~~~~~~~
# nohup lxir_run_dirac4grid_hybrpar.01 &> log &
#

echo -e "\nRunning on host `hostname -f`"
echo Time is `date`
echo Directory is `pwd`

NPROCS=`cat /proc/cpuinfo | grep processor | wc -l`
echo -e "\n This node has $NPROCS CPUs."

echo -e "\n Working server memory status"
free -m -t -h

echo -e "\n Loaded Modules"
module list

# save your PATH for later use
unset PATH_SAVED
export PATH_SAVED=$PATH

#DIRAC=/u/milias/Work/qch/software/dirac/trunk
DIRAC=/u/milias/Work/qch/software/dirac/dirac_for_grid

echo -e "\n Using installation DIRAC=$DIRAC \n"

BUILD1=$DIRAC/build_gnumkl_i8_static
PAM1=$BUILD1/pam
BUILD2=$DIRAC/build_intelmkl_i8_static/
PAM2=$BUILD2/pam

BUILD_MPI1=$DIRAC/build_intelmkl_openmpi-1.10.1_i8_static
PAM_MPI1=$BUILD_MPI1/pam
BUILD_MPI2=$DIRAC/build_openmpi_gnu_i8_openblas_static
PAM_MPI2=$BUILD_MPI2/pam

# secure for running test
export DIRTIMEOUT="12m"

# Set the basis directory environmental variable for Dirac4Grid
export BASDIR="$DIRAC/basis:$DIRAC/basis_dalton:$DIRAC/basis_ecp"

#--------------------------------------------------------------
#     Mixed parallel OpenMPI/OpenMP run on 1 node (8cpus)
#
#                Main cycle over np-MKL 
#
#          i-j: i - # for OpenMPI, j-# for OpenMP
#
#--------------------------------------------------------------
for ij in 1-1 1-2 1-4 1-8 2-1 2-2 2-4 4-1 4-2 8-1; do

  set -- ${ij//-/ }
  np=$1
  nmkl=$2

  # we have unique string of time !
  timestamp1=`date +\%F_\%k-\%M-\%S`; timestamp=${timestamp1// /};
  echo -e "\n\n Running DIRAC at "$timestamp ; echo -e "\n\n"
  export TSTDIR=DIRACtestdir.$ij.$timestamp
  export DIRAC_TMPDIR="/data.local/milias/TEST.$TSTDIR"

  unset PATH
  export PATH=$BUILD_MPI1:$PATH_SAVED
  unset OPAL_PREFIX
  export OPAL_PREFIX=$BUILD_MPI1
  unset MKL_NUM_THREADS
  export MKL_NUM_THREADS=$nmkl
  #echo "Internal MKL library parallelization with MKL_NUM_THREADS=$MKL_NUM_THREADS"
  export OMP_NUM_THREADS=1
  export MKL_DYNAMIC="FALSE"
  export OMP_DYNAMIC="FALSE"
  unset DIRAC_MPI_COMMAND
  export DIRAC_MPI_COMMAND="mpirun -np ${np}"
  echo -e "\n OpenMPI-OpenMP hybrid-parallel run OpenMPI:DIRAC_MPI_COMMAND=${DIRAC_MPI_COMMAND}; OpenMP:MKL_NUM_THREADS=${MKL_NUM_THREADS}; disk space:DIRAC_TMPDIR=${DIRAC_TMPDIR} :"
  echo -e "OpenMPI which mpirun ? \c"; which mpirun
  $PAM_MPI1 --noarch --mol=$DIRAC/test/benchmark_cc_linear/N2.ccpVTZ.mol --inp=$DIRAC/test/benchmark_cc_linear/cc.inp --suffix=out_p-im.$ij


  unset PATH
  export PATH=$BUILD_MPI2:$PATH_SAVED
  unset OPAL_PREFIX
  export OPAL_PREFIX=$BUILD_MPI2
  unset OPENBLAS_NUM_THREADS
  export OPENBLAS_NUM_THREADS=$nmkl
  unset DIRAC_MPI_COMMAND
  export DIRAC_MPI_COMMAND="mpirun -np ${np}"
  echo -e "\n OpenMPI-OpenMP hybrid-parallel run OpenMPI:DIRAC_MPI_COMMAND=${DIRAC_MPI_COMMAND}; OpenBLAS-OpenMP: disk space:DIRAC_TMPDIR=${DIRAC_TMPDIR} :"
  echo -e "OpenMPI which mpirun ? \c"; which mpirun
  $PAM_MPI2 --noarch --mol=$DIRAC/test/benchmark_cc_linear/N2.ccpVTZ.mol --inp=$DIRAC/test/benchmark_cc_linear/cc.inp --suffix=out_p-go.$ij


  #time $DIRAC/test/cc_linear/test -b $BUILD_MPI1 -w $DIRAC_TMPDIR 
  #remove scratch space afterwards - for test run only !
  #echo -e "Removing scratch dir, $DIRAC_TMPDIR"; /bin/rm -r $DIRAC_TMPDIR

done # end of the cycle over OpenMPI-OpenMP number of threads


exit 0
