#!/bin/bash

#
# script for interactive job runs on Valeria private GSI nodes
#
#  lxir022-lxir026
#  lxir071-lxir074
#
# Launched as:
# ~~~~~~~~~~~~
# nohup lxir_run_dirac4grid_hybrpar.01 &> log &
#

echo -e "\nRunning on host `hostname -f`"
echo Time is `date`
echo Directory is `pwd`

NPROCS=`cat /proc/cpuinfo | grep processor | wc -l`
echo -e "\n This node has $NPROCS CPUs."

echo -e "\n Working server memory status"
free -m -t -h

echo -e "\n Loaded Modules"
module list

# save your PATH for later use
unset PATH_SAVED
export PATH_SAVED=$PATH

#DIRAC=/u/milias/Work/qch/software/dirac/trunk
DIRAC=/u/milias/Work/qch/software/dirac/dirac_for_grid

echo -e "\n Using installation DIRAC=$DIRAC \n"

BUILD2=$DIRAC/build_gnumkl_i8_static
PAM2=$BUILD1/pam
BUILD1=$DIRAC/build_intelmkl_i8_static/
PAM1=$BUILD2/pam

BUILD_MPI1=$DIRAC/build_intelmkl_openmpi-1.10.1_i8_static
PAM_MPI1=$BUILD_MPI1/pam
BUILD_MPI2=$DIRAC/build_openmpi_gnu_i8_openblas_static
PAM_MPI2=$BUILD_MPI2/pam

# secure for running test
export DIRTIMEOUT="12m"

# Set the basis directory environmental variable for Dirac4Grid
export BASDIR="$DIRAC/basis:$DIRAC/basis_dalton:$DIRAC/basis_ecp"

#--------------------------------------------------------------
#     Mixed parallel OpenMPI/OpenMP run on 1 node (8cpus)
#
#                Main cycle over np-MKL 
#
#          i-j: i - # for OpenMPI, j-# for OpenMP
#
#--------------------------------------------------------------
#for ij in 1-1 1-2 1-4 1-8 2-1 2-2 2-4 4-1 4-2 8-1; do
#for ij in 1-1 2-1 4-1 8-1; do
for nmkl in 1 2 4 8; do

  # we have unique string of time !
  export DIRAC_TMPDIR="/data.local/milias"

  unset MKL_NUM_THREADS
  export MKL_NUM_THREADS=$nmkl
  export OMP_NUM_THREADS=1
  export MKL_DYNAMIC="FALSE"
  export OMP_DYNAMIC="FALSE"
  echo -e "\n OpenMP parallel run MKL_NUM_THREADS=${MKL_NUM_THREADS}; disk space:DIRAC_TMPDIR=${DIRAC_TMPDIR} :"
  #$PAM1 --noarch --mol=$DIRAC/test/benchmark_cc_linear/N2.ccpVTZ.mol --inp=$DIRAC/test/benchmark_cc_linear/cc.inp --suffix=out_im.$nmkl
  $PAM1 --noarch --mol=$DIRAC/test/benchmark_cc_linear/N2.ccpVQZ.mol --inp=$DIRAC/test/benchmark_cc_linear/cc.inp --suffix=out_im.$nmkl

  unset OPENBLAS_NUM_THREADS
  export OPENBLAS_NUM_THREADS=$nmkl
  echo -e "\n OpenMP parallel run  OpenBLAS: OPENBLAS_NUM_THREADS=${OPENBLAS_NUM_THREADS}; disk space:DIRAC_TMPDIR=${DIRAC_TMPDIR} :"
  #$PAM2 --noarch --mol=$DIRAC/test/benchmark_cc_linear/N2.ccpVTZ.mol --inp=$DIRAC/test/benchmark_cc_linear/cc.inp --suffix=out_go.$nmkl
  $PAM2 --noarch --mol=$DIRAC/test/benchmark_cc_linear/N2.ccpVQZ.mol --inp=$DIRAC/test/benchmark_cc_linear/cc.inp --suffix=out_go.$nmkl

  #time $DIRAC/test/cc_linear/test -b $BUILD1 -w $DIRAC_TMPDIR 
  #remove scratch space afterwards - for test run only !
  #echo -e "Removing scratch dir, $DIRAC_TMPDIR"; /bin/rm -r $DIRAC_TMPDIR

done # end of the cycle over OpenMPI-OpenMP number of threads


exit 0
