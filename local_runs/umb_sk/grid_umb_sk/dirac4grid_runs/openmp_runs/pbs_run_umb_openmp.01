#!/bin/bash

#PBS -S /bin/bash
#PBS -A UMB-ITMS-26110230082
#PBS -N D4g_omp
### Declare myprogram non-rerunable
#PBS -r n

#PBS -l nodes=1:ppn=12:old

#PBS -l walltime=20:00:00
#PBS -l mem=47g
#PBS -j oe

#PBS -q batch
##PBS -q debug

#PBS -M Miroslav.Ilias@umb.sk

echo "Working host is: "; hostname -f
source /mnt/apps/intel/bin/compilervars.sh intel64
#source /mnt/apps/pgi/environment.sh

# libnumma for PGI
#export LD_LIBRARY_PATH=/home/milias/bin/lib64_libnuma:$LD_LIBRARY_PATH

echo "My PATH=$PATH"
echo "Running on host `hostname`"
echo "Time is `date`"
echo "Directory is `pwd`"
echo "This jobs runs on the following processors:"
echo `cat $PBS_NODEFILE`

UNIQUE_NODES="`cat $PBS_NODEFILE | sort | uniq`"
UNIQUE_NODES="`echo $UNIQUE_NODES | sed s/\ /,/g `"
echo "Unique nodes for parallel run:  $UNIQUE_NODES"

# Extract number of processors
NPROCS_PBS=`wc -l < $PBS_NODEFILE`
NPROCS=`cat /proc/cpuinfo | grep processor | wc -l`
echo "This node has $NPROCS CPUs."
echo "This node has $NPROCS_PBS CPUs allocated for PBS calculations."

#echo "PBS_SERVER=$PBS_SERVER"
echo "PBS_NODEFILE=$PBS_NODEFILE"
echo "PBS_O_QUEUE=$PBS_O_QUEUE"
echo "PBS_O_WORKDIR=$PBS_O_WORKDIR"
#

#export MKL_NUM_THREADS=$NPROCS
#echo "MKL_NUM_THREADS=$MKL_NUM_THREADS"
#export MKL_DOMAIN_NUM_THREADS="MKL_BLAS=$NPROCS"
export OMP_NUM_THREADS=1
export MKL_DYNAMIC="FALSE"
export OMP_DYNAMIC="FALSE"

# provide OpenMPI-Intel
#export PATH=/home/milias/bin/openmpi-1.8.4-intel/bin:$PATH
#export LD_LIBRARY_PATH=/home/milias/bin/openmpi-1.8.4-intel/lib:$LD_LIBRARY_PATH

#DIRAC=/home/milias/Work/qch/software/dirac/trunk
#DIRAC=/home/milias/Work/qch/software/dirac/production_trunk
DIRAC=/home/milias/Work/qch/software/dirac/dirac_for_grid

#BUILD=$DIRAC/build_intel_mklpar_i8_xh_prod/
BUILD1=$DIRAC/build_intelmkl_i8_static
PAM1=$BUILD1/pam
BUILD2=$DIRAC/build_gnu_i8_openblas_static
PAM2=$BUILD2/pam

echo -e "\n The $BUILD1/dirac.x and  $BUILD2/dirac.x  attributes:"
ls -lt $BUILD1/dirac.x; ls -lt $BUILD2/dirac.x
ldd $BUILD1/dirac.x; ldd $BUILD2/dirac.x

#  set your test case
#INP=$DIRAC/test/benchmark_cc_linear/cc.inp
#MOL=$DIRAC/test/benchmark_cc_linear/N2.ccpVQZ.mol
INP=$DIRAC/test/benchmark_cc/cc.inp
MOL=$DIRAC/test/benchmark_cc/C2H4Cl2_sta_c2h.mol

# set local scratch directory for your runs 
export DIRAC_TMPDIR=/mnt/local/$USER/$PBS_JOBID

#  set BASIS directory for your runs - pam needs to see it
export BASDIR_PATH="$DIRAC/basis:$DIRAC/basis_dalton:$DIRAC/basis_ecp"

#  MPI command
#export DIRAC_MPI_COMMAND="mpirun -H ${UNIQUE_NODES} -npernode 1 -x MKL_NUM_THREADS"
#export DIRAC_MPI_COMMAND="mpirun -H ${UNIQUE_NODES} -npernode 1"

# secure maximum time for running Dirac job (for pam)
#export DIRTIMEOUT="1h"
export DIRTIMEOUT="40m"


cd $PBS_O_WORKDIR

#---------------------------------------------
#   Main cycle over MKL number of threads
#---------------------------------------------
for nmkl in 1 6 12; do

  # set MKL envirovariables
  unset MKL_NUM_THREADS
  export MKL_NUM_THREADS=$nmkl
  echo -e "\nUpdated MKL_NUM_THREADS=$MKL_NUM_THREADS"
  echo -e "MKL_DYNAMIC=$MKL_DYNAMIC"
  echo -e "OMP_NUM_THREADS=$OMP_NUM_THREADS"
  echo -e "OMP_DYNAMIC=$OMP_DYNAMIC"
  # set OpenBLAS envirovariables
  unset OPENBLAS_NUM_THREADS
  export OPENBLAS_NUM_THREADS=$nmkl

  
# send MKL enviro-variables to nodes
#PBS -v MKL_NUM_THREADS
#PBS -v MKL_DYNAMIC
#PBS -v OMP_NUM_THREADS
#PBS -v OMP_DYNAMIC
#PBS -v OPENBLAS_NUM_THREADS

# Passing your whole environment
#PBS -V

  $PAM1 --noarch --mw=400 --aw=2800 --inp=$INP  --mol=$MOL --suffix=out1n_im_omp.$nmkl
  $PAM2 --noarch --mw=400 --aw=2800 --inp=$INP  --mol=$MOL --suffix=out1n_go_omp.$nmkl

done

exit 0
