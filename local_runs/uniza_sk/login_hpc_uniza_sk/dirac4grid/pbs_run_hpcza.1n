#!/bin/sh

### Set the job name
#PBS -N D4g_1n

### Declare myprogram non-rerunable
#PBS -r n 

#PBS -q parallel

#PBS -j oe
 
#PBS -l nodes=1:ppn=12
#PBS -l mem=47g
#PBS -l walltime=200:00:00

### Switch to the working directory; by default Torque launches processes from your home directory.
### Jobs should only be run from /work; Torque returns results via NFS.
echo Working directory is $PBS_O_WORKDIR
cd $PBS_O_WORKDIR
 
### Run some informational commands.
echo Running on host `hostname`
echo Time is `date`
echo Directory is `pwd`
echo This jobs runs on the following processors:
echo `cat $PBS_NODEFILE`
echo "Unique nodes (processors), from PBS_NODEFILE:"
echo `cat $PBS_NODEFILE | sort | uniq `
 
### Define number of processors for the job
NPROCS=`wc -l < $PBS_NODEFILE`
echo This job has allocated $NPROCS cpus
echo -e "# of processors in node: \c"; cat /proc/cpuinfo | grep processor | wc -l
echo -e "The host (main) node has $NCPUS cpus."
UNIQUE_NODES="`cat $PBS_NODEFILE | sort | uniq`"
UNIQUE_NODES="`echo $UNIQUE_NODES | sed s/\ /,/g `"
echo "Unique nodes (processors) :  $UNIQUE_NODES"

export MKL_NUM_THREADS=${NPROCS}
export MKL_DYNAMIC="FALSE"
export OMP_NUM_THREADS=1
#
echo -e "\nMKL_NUM_THREADS=$MKL_NUM_THREADS"
echo -e "MKL_DYNAMIC=$MKL_DYNAMIC"
echo -e "OMP_NUM_THREADS=$OMP_NUM_THREADS"
echo -e "--------------------------------------------\n"

#. /etc/profile.d/modules.sh
#module purge
#module load intel/composerxe_2013

echo -e "\n"
pwd
df -h /localscratch
df -h .
echo -e "\n"

# save PATH
export PATH_SAVED=$PATH
echo -e "\n PATH=$PATH_SAVED"
echo -e "\n LD_LIBRARY_PATH=$LD_LIBRARY_PATH"

#DIRAC=/shared/home/ilias/Work/software/dirac/trunk4grid
DIRAC=/work/umbilias/work/software/dirac/trunk4grid
BUILD_MPI1=$DIRAC/build_intelmkl_openmpi-1.10.1_i8_static
BUILD_MPI2=$DIRAC/build_openmpi_gnu_i8_openblas_static
BUILD1=$DIRAC/build_intelmkl_i8_static
BUILD2=$DIRAC/build_gnu_i8_openblas_static

PAM_MPI1=$BUILD_MPI1/pam
PAM_MPI2=$BUILD_MPI2/pam
PAM1=$BUILD1/pam
PAM2=$BUILD2/pam

#TEST=$DIRAC/test/benchmark_cc_linear
#TEST=$DIRAC/test/cc_energy_and_mp2_dipole
TEST=$DIRAC/test/benchmark_cc
#MOL=$TEST/H2O.mol
#INP=$TEST/ccsd.inp
MOL=$TEST/C2H4Cl2_sta_c2h.mol
INP=$TEST/cc.inp

# Basis set #
export BASDIR_PATH="$DIRAC/basis:$DIRAC/basis_dalton:$DIRAC/basis_ecp"

# secure maximum time for running test
#export DIRTIMEOUT="4h"
export DIRTIMEOUT="25m"

#export MKL_NUM_THREADS=${NPROCS}

############################################
# Mixed parallel OpenMPI/OpenMP run on all nodes
############################################

#
# Main cycle ober npn-  number of MPI tasks
#
for npn in 1  3  6  9  12; do

  export NPERNODE=$npn
  export MKL_NUM_THREADS=1  # no MKL OpenMP multithreading

#PBS -v MKL_NUM_THREADS=${MKL_NUM_THREADS}

# we have unique string of time !
  timestamp1=`date +\%F_\%k-\%M-\%S`; timestamp=${timestamp1// /};
  echo -e "\n\n Running DIRAC at "$timestamp ; echo -e "\n\n"

  #export TSTDIR=DIRACtestdir.$ij.$timestamp
  export DIRAC_TMPDIR="/shared/scratch/$TSTDIR" # global scratch disk

  unset PATH
  export PATH=$BUILD_MPI1:$PATH_SAVED
  unset OPAL_PREFIX
  export OPAL_PREFIX=$BUILD_MPI1
  echo -e "which mpirun ? \c"; which mpirun
  export DIRAC_MPI_COMMAND="mpirun -H ${UNIQUE_NODES} -npernode ${NPERNODE} -x PATH"

  echo -e "\n Mixed (hybrid) OpenMP(MKL) + OpenMPI(DIRAC) parallel run, GLOBAL disk: DIRAC_MPI_COMMAND=${DIRAC_MPI_COMMAND}; MKL_NUM_THREADS=${MKL_NUM_THREADS};  DIRAC_TMPDIR=${DIRAC_TMPDIR} \n"

  $PAM_MPI1 --noarch --mw=1200 --aw=3800 --inp=$INP  --mol=$MOL --suffix=global-dsk_out1n.$npn

done # end cycle over OpenMPI-OpenMP number of threads



exit 0
